# Comparaci√≥n de 3 Metodolog√≠as de Preprocesamiento

## Objetivo

Implementar y comparar **3 metodolog√≠as DIFERENTES** de preprocesamiento de texto para cumplir con el requisito acad√©mico de aplicar m√∫ltiples enfoques de limpieza de datos en Machine Learning.

---

## Las 3 Metodolog√≠as Implementadas

### üìå Metodolog√≠a 1: TF-IDF B√°sico (Sin Limpieza NLP)

**Descripci√≥n:**
- **Preprocesamiento m√≠nimo:** Solo lowercase y eliminaci√≥n de s√≠mbolos
- **NO elimina stopwords** (the, is, and, etc.)
- **NO aplica stemming ni lemmatization**
- Enfoque minimalista para establecer baseline

**Ventajas:**
‚úÖ M√°s r√°pido computacionalmente
‚úÖ Preserva toda la informaci√≥n original
‚úÖ Simple de implementar

**Desventajas:**
‚ùå Incluye ruido (palabras comunes sin significado)
‚ùå Mayor dimensionalidad
‚ùå Posible overfitting

**Ejemplo:**
```
Original: "This wine is elegant and complex"
M√©todo 1: "this wine is elegant and complex"
         (solo lowercase)
```

---

### üìå Metodolog√≠a 2: TF-IDF + Stopwords + Stemming

**Descripci√≥n:**
- Limpieza con regex
- **Elimina stopwords** (the, is, and, etc.)
- **Aplica STEMMING** (PorterStemmer) - corta palabras a su ra√≠z

**¬øQu√© es Stemming?**
Proceso de reducir palabras a su ra√≠z mediante corte mec√°nico:
- `running` ‚Üí `run`
- `wines` ‚Üí `wine`
- `fruity` ‚Üí `fruit`

**Ventajas:**
‚úÖ Reduce dimensionalidad significativamente
‚úÖ Agrupa variantes de la misma palabra
‚úÖ Elimina palabras sin valor sem√°ntico

**Desventajas:**
‚ùå Stemming puede ser muy agresivo
‚ùå Puede perder matices sem√°nticos importantes
‚ùå Palabras irreconocibles (`complexness` ‚Üí `complex`)

**Ejemplo:**
```
Original: "This wine is elegant and complex"
M√©todo 2: "wine eleg complex"
         (stopwords eliminadas + stemming aplicado)
```

---

### üìå Metodolog√≠a 3: TF-IDF + Stopwords + Lemmatization + N-grams

**Descripci√≥n:**
- Limpieza con regex
- **Elimina stopwords**
- **Aplica LEMMATIZATION** (WordNetLemmatizer) - conversi√≥n ling√º√≠stica inteligente
- **Usa N-grams (1,2)** - captura pares de palabras para contexto

**¬øQu√© es Lemmatization?**
Proceso de convertir palabras a su forma base usando an√°lisis ling√º√≠stico:
- `running` ‚Üí `run`
- `better` ‚Üí `good`
- `wines` ‚Üí `wine`

**¬øQu√© son N-grams?**
Secuencias de N palabras consecutivas que capturan contexto:
- Unigrams (1): `["wine", "elegant", "complex"]`
- Bigrams (2): `["wine elegant", "elegant complex"]`

**Ventajas:**
‚úÖ Preserva significado ling√º√≠stico correcto
‚úÖ Captura contexto con bigramas
‚úÖ M√°s preciso sem√°nticamente
‚úÖ Suele dar mejores resultados en NLP

**Desventajas:**
‚ùå M√°s lento computacionalmente
‚ùå Mayor complejidad de implementaci√≥n
‚ùå Requiere m√°s memoria (n-grams aumentan features)

**Ejemplo:**
```
Original: "This wine is elegant and complex"
M√©todo 3: "wine elegant complex"
         + bigramas: ["wine elegant", "elegant complex"]
         (lemmatization preserva sem√°ntica + contexto capturado)
```

---

## Diferencia Clave: Stemming vs Lemmatization

| Caracter√≠stica | Stemming | Lemmatization |
|----------------|----------|---------------|
| **M√©todo** | Corte mec√°nico de sufijos | An√°lisis ling√º√≠stico |
| **Velocidad** | ‚ö° Muy r√°pido | üê¢ M√°s lento |
| **Precisi√≥n** | ‚ùå Menor | ‚úÖ Mayor |
| **Ejemplo 1** | `caring` ‚Üí `car` ‚ùå | `caring` ‚Üí `care` ‚úÖ |
| **Ejemplo 2** | `better` ‚Üí `better` | `better` ‚Üí `good` ‚úÖ |
| **Uso** | B√∫squedas r√°pidas | NLP avanzado, ML |

---

## Comparaci√≥n de Resultados

El script `train_three_methodologies.py` entrena **el mismo modelo MLP** con cada metodolog√≠a y compara:

### M√©tricas Evaluadas:

1. **MAE (Mean Absolute Error)** - Menor es mejor
   - Error promedio en la predicci√≥n
   - Ejemplo: MAE = 1.4 significa error de ¬±1.4 puntos

2. **RMSE (Root Mean Squared Error)** - Menor es mejor
   - Penaliza m√°s los errores grandes
   - M√°s sensible a outliers

3. **R¬≤ Score** - Mayor es mejor (0 a 1)
   - Qu√© tan bien el modelo explica la varianza
   - R¬≤ = 0.85 significa que explica el 85% de la variaci√≥n

4. **Tiempo de Entrenamiento** - Menor es mejor
   - Eficiencia computacional

---

## C√≥mo Ejecutar la Comparaci√≥n

### Opci√≥n 1: Archivo Batch (Windows)
```bash
run_comparison.bat
```

### Opci√≥n 2: Comando Directo
```bash
python src\models\train_three_methodologies.py
```

---

## Resultados Esperados

El script genera:

1. **Tabla comparativa en consola:**
```
RESULTADOS DE LA COMPARACI√ìN
================================================================================
                              Metodolog√≠a       MAE      RMSE        R¬≤  Tiempo (s)
 Metodolog√≠a 3: Lemmatization + N-grams     1.345     1.891     0.856      125.3
           Metodolog√≠a 2: Stemming          1.398     1.945     0.847       89.2
           Metodolog√≠a 1: TF-IDF B√°sico     1.512     2.078     0.821       56.8
================================================================================

üèÜ MEJOR METODOLOG√çA: Metodolog√≠a 3: Lemmatization + N-grams
   MAE: 1.345 puntos
```

2. **Gr√°fico comparativo:**
   - Guardado en: `docs/resultados/comparacion_metodologias.png`
   - Muestra barras de MAE y R¬≤ para cada metodolog√≠a

3. **Ejemplos de preprocesamiento:**
   - Muestra c√≥mo cada metodolog√≠a transforma el mismo texto original

---

## Conclusiones Acad√©micas

### ¬øPor qu√© 3 metodolog√≠as diferentes?

1. **Comparaci√≥n emp√≠rica:** Permite evaluar objetivamente qu√© enfoque funciona mejor
2. **An√°lisis de trade-offs:** Cada m√©todo tiene ventajas/desventajas diferentes
3. **Decisi√≥n informada:** Elegir el mejor m√©todo basado en datos, no intuici√≥n

### Hallazgos T√≠picos:

- **Metodolog√≠a 1 (B√°sico):** Baseline simple, peor performance
- **Metodolog√≠a 2 (Stemming):** Balance velocidad/precisi√≥n
- **Metodolog√≠a 3 (Lemmatization + N-grams):** Mejor precisi√≥n, m√°s costoso

### Uso en la Aplicaci√≥n:

Actualmente la GUI ([wine_ai_prophet.py](../src/gui/wine_ai_prophet.py)) usa la **Metodolog√≠a 3** (Lemmatization) porque ofrece el mejor balance entre precisi√≥n y calidad del feedback generado por IA.

---

## Referencias T√©cnicas

- **TF-IDF:** Term Frequency-Inverse Document Frequency
- **Stopwords:** Palabras comunes sin valor sem√°ntico
- **Stemming:** Algoritmo Porter Stemmer (1980)
- **Lemmatization:** WordNet Lemmatizer
- **N-grams:** Secuencias de N tokens consecutivos

---

## Para el Reporte Acad√©mico

Incluir en el documento final:

1. ‚úÖ Descripci√≥n detallada de cada metodolog√≠a
2. ‚úÖ Tabla comparativa de resultados (MAE, RMSE, R¬≤)
3. ‚úÖ Gr√°fico de barras comparativo
4. ‚úÖ Ejemplo visual de transformaci√≥n de texto
5. ‚úÖ Justificaci√≥n de la elecci√≥n de la mejor metodolog√≠a
6. ‚úÖ An√°lisis de trade-offs (precisi√≥n vs velocidad)

---


